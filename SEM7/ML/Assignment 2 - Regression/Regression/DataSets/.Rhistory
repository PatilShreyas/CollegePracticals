# Function that returns Root Mean Squared Error
rmse <- function(error)
{
sqrt(mean(error^2))
}
# Function that returns Mean Absolute Error
mae <- function(error)
{
mean(abs(error))
}
#calculate Training Error
trainerror <- Trainactual - Trainpredicted
TrainMSE <- rmse(trainerror)
#calculate TestingError
testerror <- Testactual - Testpredicted
TestMSE <- rmse(testerror)
#Comparison between models
fit2 <- lm(y~poly(x,2,raw=TRUE))
fit3 <- lm(y~poly(x,3,raw=TRUE))
fit4 <- lm(y~poly(x,4,raw=TRUE))
fit5 <- lm(y~poly(x,5,raw=TRUE))
fit6 <- lm(y~poly(x,6,raw=TRUE))
fit7 <- lm(y~poly(x,7,raw=TRUE))
fit8 <- lm(y~poly(x,8,raw=TRUE))
fit9 <- lm(y~poly(x,9,raw=TRUE))
anova(fit,fit2,fit3,fit4,fit5,fit6,fit7,fit8,fit9)
#Calcualte trainerror ans test error for each model
#Linear Model
fittrainerror <- train$x - (predict.lm(fit,data.frame(x=train$x)))
fittesterror <- test$y - (predict.lm(fit,data.frame(x=test$x)))
fittrainmse <- rmse(fittrainerror)
fittestmse <- rmse(fittesterror)
#Polynomial Model with degree 2
fit2trainerror <- train$y - (predict.lm(fit2,data.frame(x=train$x)))
fit2testerror <- test$y - (predict.lm(fit2,data.frame(x=test$x)))
fit2trainmse <- rmse(fit2trainerror)
fit2testmse <- rmse(fit2testerror)
#Polynomial Model with degree 3
fit3trainerror <- train$y - (predict.lm(fit3,data.frame(x=train$x)))
fit3testerror <- test$y - (predict.lm(fit3,data.frame(x=test$x)))
fit3trainmse <- rmse(fit3trainerror)
fit3testmse <- rmse(fit3testerror)
#Polynomial Model with degree 4
fit4trainerror <- train$y - (predict.lm(fit4,data.frame(x=train$x)))
fit4testerror <- test$y - (predict.lm(fit4,data.frame(x=test$x)))
fit4trainmse <- rmse(fit4trainerror)
fit4testmse <- rmse(fit4testerror)
#comparison of Train and Test error for each model
analysis <- cbind(fittrainmse,fittestmse,fit2trainmse,fit2testmse,fit3trainmse,fit3testmse,fit4trainmse,fit4testmse)
analysis
#plot graph for predicted models
plot(data$x,data$y,pch=5) #scatter plot for pizza.csv dataset
lines(x,predict.lm(fit,data.frame(x=x)),col='Red')
lines(x,predict.lm(fit2,data.frame(x=x)),col='Green')
lines(x,predict.lm(fit3,data.frame(x=x)),col='Blue')
lines(x,predict.lm(fit4,data.frame(x=x)),col='yellow')
#k-fold cross Validation
library(DAAG)
val.daag <-cv.lm(df=data,form.lm=lm(y ~ x))
#Read dataset from csv
data<- read.csv("F:\machine learning\ML Assignments(1)\ML Assignments\Linear Regression_ERROR/pizzaa.csv")
#split dataset into Training and Testing Dataset
dim(data)
indexes = sample(1:nrow(data), size=0.2*nrow(data))
test = data[indexes,]
dim(test)
train = data[-indexes,]
dim(train)
#Linear Regression Model
x<- train$x
y<- train$y
plot(x, y)
fit <- lm(y ~ x)
abline(fit)
fit
summary(fit)
#calculation of Test Mse and Train MSE
Testactual <- test$x
Testpredicted <- predict.lm(fit,data.frame(x=test$x))
Trainactual <- train$y
Trainpredicted <- predict.lm(fit,data.frame(x=train$y))
# Function that returns Root Mean Squared Error
rmse <- function(error)
{
sqrt(mean(error^2))
}
# Function that returns Mean Absolute Error
mae <- function(error)
{
mean(abs(error))
}
#calculate Training Error
trainerror <- Trainactual - Trainpredicted
TrainMSE <- rmse(trainerror)
#calculate TestingError
testerror <- Testactual - Testpredicted
TestMSE <- rmse(testerror)
#Comparison between models
fit2 <- lm(y~poly(x,2,raw=TRUE))
fit3 <- lm(y~poly(x,3,raw=TRUE))
fit4 <- lm(y~poly(x,4,raw=TRUE))
fit5 <- lm(y~poly(x,5,raw=TRUE))
fit6 <- lm(y~poly(x,6,raw=TRUE))
fit7 <- lm(y~poly(x,7,raw=TRUE))
fit8 <- lm(y~poly(x,8,raw=TRUE))
fit9 <- lm(y~poly(x,9,raw=TRUE))
anova(fit,fit2,fit3,fit4,fit5,fit6,fit7,fit8,fit9)
#Calcualte trainerror ans test error for each model
#Linear Model
fittrainerror <- train$x - (predict.lm(fit,data.frame(x=train$x)))
fittesterror <- test$y - (predict.lm(fit,data.frame(x=test$x)))
fittrainmse <- rmse(fittrainerror)
fittestmse <- rmse(fittesterror)
#Polynomial Model with degree 2
fit2trainerror <- train$y - (predict.lm(fit2,data.frame(x=train$x)))
fit2testerror <- test$y - (predict.lm(fit2,data.frame(x=test$x)))
fit2trainmse <- rmse(fit2trainerror)
fit2testmse <- rmse(fit2testerror)
#Polynomial Model with degree 3
fit3trainerror <- train$y - (predict.lm(fit3,data.frame(x=train$x)))
fit3testerror <- test$y - (predict.lm(fit3,data.frame(x=test$x)))
fit3trainmse <- rmse(fit3trainerror)
fit3testmse <- rmse(fit3testerror)
#Polynomial Model with degree 4
fit4trainerror <- train$y - (predict.lm(fit4,data.frame(x=train$x)))
fit4testerror <- test$y - (predict.lm(fit4,data.frame(x=test$x)))
fit4trainmse <- rmse(fit4trainerror)
fit4testmse <- rmse(fit4testerror)
#comparison of Train and Test error for each model
analysis <- cbind(fittrainmse,fittestmse,fit2trainmse,fit2testmse,fit3trainmse,fit3testmse,fit4trainmse,fit4testmse)
analysis
#plot graph for predicted models
plot(data$x,data$y,pch=5) #scatter plot for pizza.csv dataset
lines(x,predict.lm(fit,data.frame(x=x)),col='Red')
lines(x,predict.lm(fit2,data.frame(x=x)),col='Green')
lines(x,predict.lm(fit3,data.frame(x=x)),col='Blue')
lines(x,predict.lm(fit4,data.frame(x=x)),col='yellow')
#k-fold cross Validation
library(DAAG)
val.daag <-cv.lm(df=data,form.lm=lm(y ~ x))
>val.daag <- cv.lm(df=data,form.lm=lm(y ~ x))
n <- 150 # number of data points
p <- 2 # dimension
sigma <- 1 # variance of the distribution
meanpos <- 0 # centre of the distribution of positive examples
meanneg <- 3 # centre of the distribution of negative examples
npos <- round(n/2) # number of positive examples
nneg <- n-npos # number of negative examples
# Generate the positive and negative examples
xpos <- matrix(rnorm(npos*p,mean=meanpos,sd=sigma),npos,p)
xneg <- matrix(rnorm(nneg*p,mean=meanneg,sd=sigma),npos,p)
x <- rbind(xpos,xneg)
# Generate the labels
y <- matrix(c(rep(1,npos),rep(-1,nneg)))
# Visualize the data
plot(x,col=ifelse(y>0,1,2))
legend("topleft",c('Positive','Negative'),col=seq(2),pch=1,text.col=seq(2))
## Prepare a training and a test set ##
ntrain <- round(n*0.8) # number of training examples
tindex <- sample(n,ntrain) # indices of training samples
xtrain <- x[tindex,]
xtest <- x[-tindex,]
ytrain <- y[tindex]
ytest <- y[-tindex]
istrain=rep(0,n)
istrain[tindex]=1
# load the kernlab package
library(kernlab)
# train the SVM
svp <- ksvm(xtrain,ytrain,type="C-svc",kernel='vanilladot',C=100,scaled=c())
# General summary
svp
# Attributes that you can access
attributes(svp)
# For example, the support vectors
alpha(svp)
alphaindex(svp)
b(svp)
# Use the built-in function to pretty-plot the classifier
plot(svp,data=xtrain)
# Predict labels on test
ypred = predict(svp,xtest)
table(ytest,ypred)
# Compute accuracy
sum(ypred==ytest)/length(ytest)
# Compute at the prediction scores
ypredscore = predict(svp,xtest,type="decision")
# Check that the predicted labels are the signs of the scores
table(ypredscore > 0,ypred)
# Package to compute ROC curve, precision-recall etc...
library(ROCR)
pred <- prediction(ypredscore,ytest)
# Plot ROC curve
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(perf)
# Plot precision/recall curve
perf <- performance(pred, measure = "prec", x.measure = "rec")
plot(perf)
# Plot accuracy as function of threshold
perf <- performance(pred, measure = "acc")
plot(perf)
cv.folds <- function(n,folds=3)
## randomly split the n samples into folds
{
split(sample(n),rep(1:folds,length=length(y)))
}
## the kNN classifier
library (class)
data(iris3)
names(iris3)
# 1. using a separate test set
train <- rbind(iris3[1:25,,1], iris3[1:25,,2], iris3[1:25,,3])
test <- rbind(iris3[26:50,,1], iris3[26:50,,2], iris3[26:50,,3])
cl <- factor(c(rep("s",25), rep("c",25), rep("v",25)))
myknn <- knn(train, test, cl, k = 3, prob=TRUE)
attributes(.Last.value)
tab <- table(myknn, cl)
sum(tab[row(tab)==col(tab)])/sum(tab)
tab
# one can use 'knn1' when k=1
# 2. using LOOCV
train <- rbind(iris3[,,1], iris3[,,2], iris3[,,3])
cl <- factor(c(rep("s",50), rep("c",50), rep("v",50)))
myknn.cv <- knn.cv(train, cl, k = 3, prob = TRUE)
tab <- table(myknn.cv, cl)
sum(tab[row(tab)==col(tab)])/sum(tab)
tab
install.packages("F:/machine learning/ML Assignments(1)/e1071/R/e1071.rdb", repos = NULL)
library(e1071)
data(iris)
m <- naiveBayes (Species ~ ., data = iris)
tab <- table(predict(m, iris[,-5]), iris[,5])
sum(tab[row(tab)==col(tab)])/sum(tab)
### R code from vignette source 'svmdoc.Rnw'
###################################################
### code chunk number 1: svmdoc.Rnw:140-149
###################################################
library(e1071)
library(rpart)
data(Glass, package="mlbench")
## split data into a train and test set
index     <- 1:nrow(Glass)
testindex <- sample(index, trunc(length(index)/3))
testset   <- Glass[testindex,]
trainset  <- Glass[-testindex,]
###################################################
### code chunk number 2: svmdoc.Rnw:154-157
###################################################
## svm
svm.model <- svm(Type ~ ., data = trainset, cost = 100, gamma = 1)
svm.pred  <- predict(svm.model, testset[,-10])
###################################################
### code chunk number 3: svmdoc.Rnw:162-165
###################################################
## rpart
rpart.model <- rpart(Type ~ ., data = trainset)
rpart.pred  <- predict(rpart.model, testset[,-10], type = "class")
###################################################
### code chunk number 4: svmdoc.Rnw:168-173
###################################################
## compute svm confusion matrix
table(pred = svm.pred, true = testset[,10])
## compute rpart confusion matrix
table(pred = rpart.pred, true = testset[,10])
###################################################
### code chunk number 5: svmdoc.Rnw:178-213
###################################################
library(xtable)
rp.acc <- c()
sv.acc <- c()
rp.kap <- c()
sv.kap <- c()
reps <- 10
for (i in 1:reps) {
## split data into a train and test set
index     <- 1:nrow(Glass)
testindex <- sample(index, trunc(length(index)/3))
testset   <- na.omit(Glass[testindex,])
trainset  <- na.omit(Glass[-testindex,])
## svm
svm.model <- svm(Type ~ ., data = trainset, cost = 100, gamma = 1)
svm.pred  <- predict(svm.model, testset[,-10])
tab <- classAgreement(table(svm.pred, testset[,10]))
sv.acc[i] <- tab$diag
sv.kap[i] <- tab$kappa
## rpart
rpart.model <- rpart(Type ~ ., data = trainset)
rpart.pred  <- predict(rpart.model, testset[,-10], type = "class")
tab <- classAgreement(table(rpart.pred, testset[,10]))
rp.acc[i] <- tab$diag
rp.kap[i] <- tab$kappa
}
x <- rbind(summary(sv.acc), summary(sv.kap), summary(rp.acc), summary(rp.kap))
rownames <- c()
tab <- cbind(rep(c("svm","rpart"),2), round(x,2))
colnames(tab)[1] <- "method"
rownames(tab) <- c("Accuracy","","Kappa"," ")
xtable(tab, label = "tab:class", caption = "Performance of \\texttt{svm()} and\
\\texttt{rpart()} for classification (10 replications)")
###################################################
### code chunk number 6: svmdoc.Rnw:226-245
###################################################
library(e1071)
library(rpart)
data(Ozone, package="mlbench")
## split data into a train and test set
index     <- 1:nrow(Ozone)
testindex <- sample(index, trunc(length(index)/3))
testset   <- na.omit(Ozone[testindex,-3])
trainset  <- na.omit(Ozone[-testindex,-3])
## svm
svm.model <- svm(V4 ~ ., data = trainset, cost = 1000, gamma = 0.0001)
svm.pred  <- predict(svm.model, testset[,-3])
crossprod(svm.pred - testset[,3]) / length(testindex)
## rpart
rpart.model <- rpart(V4 ~ ., data = trainset)
rpart.pred  <- predict(rpart.model, testset[,-3])
crossprod(rpart.pred - testset[,3]) / length(testindex)
###################################################
### code chunk number 7: svmdoc.Rnw:248-271
###################################################
rp.res <- c()
sv.res <- c()
reps <- 10
for (i in 1:reps) {
## split data into a train and test set
index     <- 1:nrow(Ozone)
testindex <- sample(index, trunc(length(index)/3))
testset   <- na.omit(Ozone[testindex,-3])
trainset  <- na.omit(Ozone[-testindex,-3])
## svm
svm.model <- svm(V4 ~ ., data = trainset, cost = 1000, gamma = 0.0001)
svm.pred  <- predict(svm.model, testset[,-3])
sv.res[i] <- crossprod(svm.pred - testset[,3]) / length(testindex)
## rpart
rpart.model <- rpart(V4 ~ ., data = trainset)
rpart.pred  <- predict(rpart.model, testset[,-3])
rp.res[i] <- crossprod(rpart.pred - testset[,3]) / length(testindex)
}
xtable(rbind(svm = summary(sv.res), rpart = summary(rp.res)),
label = "tab:reg", caption = "Performance of \\texttt{svm()} and\
\\texttt{rpart()} for regression (Mean Squared Error, 10 replications)")
install.packages("e1071")
library(e1071)
data(iris)
m <- naiveBayes (Species ~ ., data = iris)
tab <- table(predict(m, iris[,-5]), iris[,5])
sum(tab[row(tab)==col(tab)])/sum(tab)
install.packages("e1071")
library(e1071)
data(iris)
m <- naiveBayes (Species ~ ., data = iris)
tab <- table(predict(m, iris[,-5]), iris[,5])
sum(tab[row(tab)==col(tab)])/sum(tab)
install.packages("e1071")
library(e1071)
data(iris)
m <- naiveBayes (Species ~ ., data = iris)
tab <- table(predict(m, iris[,-5]), iris[,5])
sum(tab[row(tab)==col(tab)])/sum(tab)
install.packages("e1071")
install.packages("e1071")
install.packages("e1071")
library(e1071)
data(iris)
m <- naiveBayes (Species ~ ., data = iris)
tab <- table(predict(m, iris[,-5]), iris[,5])
sum(tab[row(tab)==col(tab)])/sum(tab)
install.packages("e1071")
library("e1071", lib.loc="~/R/win-library/3.3")
install.packages("e1071")
install.packages("e1071")
library(e1071)
data(iris)
m <- naiveBayes (Species ~ ., data = iris)
tab <- table(predict(m, iris[,-5]), iris[,5])
sum(tab[row(tab)==col(tab)])/sum(tab)
install.packages("e1071")
library(e1071)
data(iris)
m <- naiveBayes (Species ~ ., data = iris)
tab <- table(predict(m, iris[,-5]), iris[,5])
sum(tab[row(tab)==col(tab)])/sum(tab)
install.packages("e1071")
install.packages("C:\Users\snehal123\AppData\Local\Temp\RtmpKEAB4R\downloaded_packages")
## Example of using a contingency table:
data(Titanic)
m <- naiveBayes(Survived ~ ., data = Titanic)
m
predict(m, as.data.frame(Titanic))
## Example with metric predictors:
data(iris)
m <- naiveBayes(Species ~ ., data = iris)
## alternatively:
m <- naiveBayes(iris[,-5], iris[,5])
m
table(predict(m, iris), iris[,5])
## Categorical data only:
data(HouseVotes84, package = "mlbench")
model <- naiveBayes(Class ~ ., data = HouseVotes84)
predict(model, HouseVotes84[1:10,])
predict(model, HouseVotes84[1:10,], type = "raw")
pred <- predict(model, HouseVotes84)
table(pred, HouseVotes84$Class)
## using laplace smoothing:
model <- naiveBayes(Class ~ ., data = HouseVotes84, laplace = 3)
pred <- predict(model, HouseVotes84[,-1])
table(pred, HouseVotes84$Class)
## Example of using a contingency table:
data(Titanic)
m <- naiveBayes(Survived ~ ., data = Titanic)
m
predict(m, as.data.frame(Titanic))
## Example with metric predictors:
data(iris)
m <- naiveBayes(Species ~ ., data = iris)
## alternatively:
m <- naiveBayes(iris[,-5], iris[,5])
m
table(predict(m, iris), iris[,5])
#PART 1 : Naive Bayes Classification
# data set to be used is Housevotes84 in the package mlbench so install
# it first...it will take few minutes wait..
#mlbench is required for data set HouseVote84
install.packages("mlbench")
library("mlbench", lib.loc="~/R/win-library/3.1")
data(HouseVotes84)
# VIEW  DATA_SET
head(HouseVotes84,8)
#NOTE THAT NAIVEE BAYES ALLOWS MISSING VALUES
#naivebayes requires e1071
install.packages("e1071")
library(e1071)
# dividing input data into training data and test data
# number of rows in HouseVotes84 are 435 ...
#let first 400 are training data and last 35 are test data
trainingdata <- head(HouseVotes84,400)
testdata<-head(HouseVotes84,-400)
#check dimensions
dim(HouseVotes84)
dim(trainingdata)
dim(testdata)
#construct classifier by naive bayes classifier.
modelposteriorprob <- naiveBayes(Class ~ ., data = trainingdata, type = "raw")
modelposteriorprob
nbmodel <- naiveBayes(Class ~ ., data = trainingdata)
#print the model
nbmodel
# applying model to test data to decide the class of each record in test data
#predict(model, trainingdata)
# applying model to test data to decide POSTERIOR PROBABILITY FOR EACH CLASS & each record in test data
nbresult<-predict(model, testdata)
nbresult
# CARET PACKAGE IS REQUIRED FOR CONFUSION MATRIX
install.packages(caret)
library(caret)
nbcm<-confusionMatrix(result, testdata$Class)
#know ur confusion matrix
str(nbcm)
#alternaet way to calculate confusion matrix
tab0<-table(result,testdata$Class)
tab0
diag(tab0)/apply(tab0, 2, sum)
#####PART 2 : KNN CLASSIFIER ###############
####### K NEAREST CLASSIFIER ###########
# WE NEED TRAINING DATA & TEST DATA
# lets revisit training data and test data
trainingdata
testdata
##NOTE THAT NAIVEE BAYES DOES NOT ALLOW MISSING VALUES
#consider another dataset without missing values
library (class)
install.packages("class")
train <- rbind(iris3[1:25,,1], iris3[1:25,,2], iris3[1:25,,3])
train
test <- rbind(iris3[26:50,,1], iris3[26:50,,2], iris3[26:50,,3])
test
#LETUS NOW DIMENSIONS
dim(iris3)
dim(train)
dim(test)
cl <- factor(c(rep("s",25), rep("c",25), rep("v",25)))
myknn <- knn(train, test, cl, k = 3)
myknn
attributes(.Last.value)
#confusion matrix
tab <- table(myknn, cl)
tab
############### end of KNN & NAIVE BAYES CLASSIFIER ###############
install.packages("class")
q()
rm(list=ls())
setwd("F:/Rajasree/R/2019_20SEMI/ML/Regression/DataSets")
setwd("F:/Rajasree/R/2019_20SEMI/ML/Regression/DataSets")
input <- read.csv("F:/Rajasree/R/2019_20SEMI/ML/Regression/DataSets/input.csv")
attach(input)
source('C:/Users/SNEHAL~1/AppData/Local/Temp/Rar$DIa7652.13305/regressionINPUT.R')
pt1 <- locator(1)
source('F:/Rajasree/R/2019_20SEMI/ML/Regression/DataSets/regressionsample.R')
source('F:/Rajasree/R/2019_20SEMI/ML/Regression/DataSets/regressionsample.R')
source('F:/Rajasree/R/2019_20SEMI/ML/Regression/DataSets/regressionsample.R')
names(input)
source('F:/Rajasree/R/2019_20SEMI/ML/Regression/DataSets/regressionsample.R')
source('F:/Rajasree/R/2019_20SEMI/ML/Regression/DataSets/regressionsample.R')
rm(list=ls())
input <- read.csv("F:/Rajasree/R/2019_20SEMI/ML/Regression/DataSets/sample.csv")
attach(input)
attach(input)
names(input)
head(input,8)
plot(runs~at_bats,main="Runs Vs At_bats",xlim=c(1,6),ylim=c(1,6))
cor(runs,at_bats)
